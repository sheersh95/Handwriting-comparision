{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math\n",
    "import sklearn.utils as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Observed Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''with open(\"HumanObserved-Features-Data.csv\", \"r\") as f: #Read features csv\n",
    "    x = pd.read_csv(f)\n",
    "    \n",
    "with open(\"diffn_pairs.csv\", \"r\") as g: # Read Different pairs csv\n",
    "    y = pd.read_csv(g)\n",
    "    \n",
    "with open(\"same_pairs.csv\", \"r\") as g:  #Read same pairs csv\n",
    "    z = pd.read_csv(g)\n",
    "    \n",
    "y = y.reindex(np.random.permutation(y.index))  # Randomize the csv file\n",
    "y1 = y[0:2500]\n",
    "Raw = pd.concat([y1,z],ignore_index = True)\n",
    "Raw = Raw.reindex(np.random.permutation(Raw.index))\n",
    "\n",
    "#Concatenation\n",
    "df_merge = pd.merge(Raw,x,left_on='img_id_A',right_on='img_id')\n",
    "df_merge=pd.merge(df_merge,x,left_on='img_id_B',right_on='img_id') \n",
    "df_merge=df_merge.drop(df_merge.columns[[0,1,3,4,14,15]],axis=1)\n",
    "df_merge = df_merge.loc[:,(df_merge!=0).any(axis=0)] #Remove zero variance features\n",
    "df_merge.to_csv('HumanFeatureData.csv')\n",
    "\n",
    "#Subtraction\n",
    "dfA=pd.merge(Raw,x,left_on='img_id_A',right_on='img_id')\n",
    "dfB=pd.merge(Raw,x,left_on='img_id_B',right_on='img_id')\n",
    "dfA=dfA.drop(dfA.columns[[0,1,2,3,4]],axis=1)\n",
    "dfB=dfB.drop(dfB.columns[[0,1,2,3,4]],axis=1)\n",
    "dfSub=abs(dfA-dfB)\n",
    "dfSub=Raw.merge(dfSub,how='outer',left_index=True, right_index=True)\n",
    "dfSub=dfSub.drop(dfSub.columns[[0,1]],axis=1)\n",
    "df_Sub = df_Sub.loc[:,(df_Sub!=0).any(axis=0)] #Remove zero variance features\n",
    "dfSub.to_csv('HumanFeatureDataSub.csv')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSC Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''with open(\"GSC-Features.csv\", \"r\") as f:  #Read features csv\n",
    "    x = pd.read_csv(f)\n",
    "    \n",
    "with open(\"diffn_pairs_gsc.csv\", \"r\") as g:  # Read Different pairs csv\n",
    "    y = pd.read_csv(g)\n",
    "    \n",
    "with open(\"same_pairs_gsc.csv\", \"r\") as g: #Read same pairs csv\n",
    "    z = pd.read_csv(g)\n",
    "    \n",
    "y = y.reindex(np.random.permutation(y.index))  # Randomize the two csv files\n",
    "z = z.reindex(np.random.permutation(z.index))\n",
    "y1 = y[0:2000]\n",
    "z1 = z[0:1000]  \n",
    "Raw = pd.concat([y1,z1],ignore_index = True)  #Concatenate the two csv files\n",
    "Raw = Raw.reindex(np.random.permutation(Raw.index))  #Randomize the csv file\n",
    "\n",
    "#Concatenation\n",
    "df_merge = pd.merge(Raw,x,left_on='img_id_A',right_on='img_id')\n",
    "df_merge=pd.merge(df_merge,x,left_on='img_id_B',right_on='img_id') \n",
    "df_merge=df_merge.drop(df_merge.columns[[0,1,3,516]],axis=1)\n",
    "df_merge = df_merge.loc[:,(df_merge!=0).any(axis=0)] #Remove zero variance features\n",
    "df_merge.to_csv('GSCFeatureData.csv')\n",
    "\n",
    "#Subtraction\n",
    "dfA=pd.merge(Raw,x,left_on='img_id_A',right_on='img_id')\n",
    "dfB=pd.merge(Raw,x,left_on='img_id_B',right_on='img_id')\n",
    "dfA=dfA.drop(dfA.columns[[0,1,2,3]],axis=1)\n",
    "dfB=dfB.drop(dfB.columns[[0,1,2,3]],axis=1)\n",
    "dfSub=abs(dfA-dfB)\n",
    "dfSub=Raw.merge(dfSub,how='outer',left_index=True, right_index=True)\n",
    "dfSub=dfSub.drop(dfSub.columns[[0,1]],axis=1)\n",
    "dfSub = dfSub.loc[:,(dfSub!=0).any(axis=0)]  #Remove zero variance features\n",
    "dfSub.to_csv('GSCFeatureDataSub.csv')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingPercent = 80 #Training split\n",
    "ValidationPercent = 10 #Validation split\n",
    "TestPercent = 10 #Test split\n",
    "LearningRate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "507"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = pd.read_csv('GSCFeatureDataSub.csv')\n",
    "lenc = len(file.columns) #To get the number of features\n",
    "lenc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to Generate the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateRawData(file_path): #Generate the Raw Feature Matrix and Target Vector\n",
    "    data_target = []\n",
    "    data_input = []\n",
    "    x = pd.read_csv(file_path)\n",
    "    \n",
    "    data_target = x['target'].tolist()\n",
    "\n",
    "    data_input = x.iloc[:,2:lenc]\n",
    "    \n",
    "    data = np.array(data_input).T\n",
    "    target = data_target\n",
    "\n",
    "    return data,target\n",
    "\n",
    "def GenerateTrainingTarget(rawTraining,TrainingPercent = 80):  #Splitting data to get Training target data\n",
    "    TrainingLen = int(math.ceil(len(rawTraining)*(TrainingPercent*0.01)))\n",
    "    t           = rawTraining[:TrainingLen]\n",
    "    return t\n",
    "\n",
    "def GenerateTrainingDataMatrix(rawData, TrainingPercent = 80): #Splitting data to get Training feature data\n",
    "    T_len = int(math.ceil(len(rawData[0])*0.01*TrainingPercent))\n",
    "    d2 = rawData[:,0:T_len]\n",
    "    return d2\n",
    "\n",
    "def GenerateValData(rawData, ValPercent, TrainingCount): #Splitting data to get Validation feature data\n",
    "    valSize = int(math.ceil(len(rawData[0])*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize\n",
    "    dataMatrix = rawData[:,TrainingCount+1:V_End]  \n",
    "    return dataMatrix\n",
    "\n",
    "def GenerateValTargetVector(rawData, ValPercent, TrainingCount): #Splitting data to get Validation target data\n",
    "    valSize = int(math.ceil(len(rawData)*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize\n",
    "    t =rawData[TrainingCount+1:V_End]\n",
    "    return t\n",
    "\n",
    "def Sigmoid(value):  #Function to Calculate the sigmoid of a value\n",
    "    return 1 / (1 + np.exp(-value))\n",
    "\n",
    "def predictY(Data,W):  #Function to predict the Target Value\n",
    "    val = np.dot(W.T,Data)\n",
    "    return Sigmoid(val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RawData,RawTarget = GenerateRawData('HumanFeatureData.csv') #Function call to make Feature Matrix and Target Vector\n",
    "#RawData,RawTarget = GenerateRawData('HumanFeatureDataSub.csv')\n",
    "RawData,RawTarget = GenerateRawData('GSCFeatureData.csv')\n",
    "#RawData,RawTarget = GenerateRawData('GSCFeatureDataSub.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400,)\n",
      "(505, 2400)\n"
     ]
    }
   ],
   "source": [
    "TrainingTarget = np.array(GenerateTrainingTarget(RawTarget,TrainingPercent)) #Prepare the target vector for training\n",
    "#dataset\n",
    "TrainingData   = GenerateTrainingDataMatrix(RawData,TrainingPercent) #Prepare the Feature matrix for training dataset\n",
    "print(TrainingTarget.shape)\n",
    "print(TrainingData.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(299,)\n",
      "(505, 299)\n"
     ]
    }
   ],
   "source": [
    "ValDataAct = np.array(GenerateValTargetVector(RawTarget,ValidationPercent, (len(TrainingTarget)))) #Prepare the \n",
    "#Validation target vector\n",
    "ValData    = GenerateValData(RawData,ValidationPercent, (len(TrainingTarget)))# Prepare the feature matrix for \n",
    "#Validation Dataset\n",
    "print(ValDataAct.shape)\n",
    "print(ValData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(299,)\n",
      "(505, 299)\n"
     ]
    }
   ],
   "source": [
    "TestDataAct = np.array(GenerateValTargetVector(RawTarget,TestPercent, (len(TrainingTarget)+len(ValDataAct))))#Prepare\n",
    "#the Target vector for testing dataset\n",
    "TestData = GenerateValData(RawData,TestPercent, (len(TrainingTarget)+len(ValDataAct)))# Prepare the feature matrix \n",
    "#for Testing datasewt\n",
    "print(TestDataAct.shape)\n",
    "print(TestData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    " W = np.ones((lenc-2,1)) #Initialize the weights\n",
    "\n",
    "for itera in range(0,1000):\n",
    "    PredictY = predictY(TrainingData,W)  #Predict target values\n",
    "    Sub = PredictY - TrainingTarget   #This is to calculate the Gradient.\n",
    "    Delta_w = np.dot(TrainingData,Sub.T)\n",
    "    Delta_w = Delta_w/len(TrainingData)\n",
    "    Delta_w = Delta_w*LearningRate  #Gradient Calculated\n",
    "    W = W - Delta_w  #Updating the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61125\n"
     ]
    }
   ],
   "source": [
    "#Calculate Training Accuracy\n",
    "TR_Y = Sigmoid(np.dot(np.transpose(W),TrainingData)) \n",
    "Predicted = np.round(TR_Y)\n",
    "TrAccuracy = (TrainingTarget == Predicted).sum()\n",
    "TrAccuracy = TrAccuracy/len(TrainingTarget)\n",
    "print(TrAccuracy)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run for Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,1000):  #Run Iterations for validation\n",
    "    PredictY = predictY(ValData,W)\n",
    "    Sub = PredictY - ValDataAct\n",
    "    Delta_w = np.dot(ValData,Sub.T)\n",
    "    Delta_w = Delta_w/len(ValData)\n",
    "    Delta_w = Delta_w*LearningRate\n",
    "    W = W - Delta_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5183946488294314\n"
     ]
    }
   ],
   "source": [
    "#Calculate Validation Accuracy\n",
    "V_Y = Sigmoid(np.dot(np.transpose(W),ValData)) \n",
    "Predicted = np.round(V_Y)\n",
    "VAccuracy = (ValDataAct == Predicted).sum()\n",
    "VAccuracy = VAccuracy/len(ValDataAct)\n",
    "print(VAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4682274247491639\n"
     ]
    }
   ],
   "source": [
    "#Calculate Testing Accuracy\n",
    "T_Y = Sigmoid(np.dot(np.transpose(W),TestData)) \n",
    "Predicted = np.round(T_Y)\n",
    "TAccuracy = (TestDataAct == Predicted).sum()\n",
    "TAccuracy = TAccuracy/len(TestDataAct)\n",
    "print(TAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
